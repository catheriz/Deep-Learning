{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73744a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "%tensorflow_version 1.15\n",
    "\n",
    "#print(keras.__version__)\n",
    "\n",
    "if (tf.__version__.split('.')[0] == '2'):\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_v2_behavior()\n",
    "\n",
    "# Load MNIST dataset\n",
    "import input_data\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  # call mnist function\n",
    "\n",
    "learningRate = 1e-3\n",
    "trainingIters = 150000\n",
    "batchSize = 50\n",
    "displayStep = 1000\n",
    "\n",
    "nInput = 28  # we want the input to take the 28 pixels\n",
    "nSteps = 28  # every 28\n",
    "nHidden = 50  # number of neurons for the RNN\n",
    "nClasses = 10  # this is MNIST so you know\n",
    "\n",
    "x = tf.placeholder('float', [None, nSteps, nInput])\n",
    "y = tf.placeholder('float', [None, nClasses])\n",
    "\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([nHidden, nClasses]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([nClasses]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    x = tf.reshape(x, [-1, nInput])\n",
    "    x = tf.split(x, nSteps, 0)  # configuring so you can get it as needed for the 28 pixels\n",
    "\n",
    "    # find which lstm to use in the documentation\n",
    "    # lstmCell = rnn_cell.BasicLSTMCell(nHidden, forget_bias=1.0)\n",
    "    # lstmCell = rnn_cell.GRUCell(nHidden)\n",
    "    lstmCell = rnn_cell.BasicRNNCell(num_units = nHidden)\n",
    "\n",
    "    outputs, states = rnn.static_rnn(lstmCell, x,\n",
    "                                     dtype=tf.float32)  # for the rnn where to get the output and hidden state\n",
    "\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# optimization\n",
    "# create the cost, optimization, evaluation, and accuracy\n",
    "# for the cost softmax_cross_entropy_with_logits seems really good\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learningRate).minimize(cost)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "\n",
    "    while step * batchSize < trainingIters:\n",
    "        batchX, batchY = mnist.train.next_batch(batchSize)  # mnist has a way to get the next batch\n",
    "        batchX = batchX.reshape((batchSize, nSteps, nInput))\n",
    "\n",
    "        sess.run(optimizer, feed_dict={x: batchX, y: batchY})\n",
    "\n",
    "        acc = sess.run(accuracy, feed_dict={x: batchX, y: batchY})\n",
    "        loss = sess.run(cost, feed_dict={x: batchX, y: batchY})\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        accuracy_list.append(acc)\n",
    "\n",
    "        if step % displayStep == 0:\n",
    "            print(\"Iter \" + str(step * batchSize) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print('Optimization finished')\n",
    "\n",
    "    testData = mnist.test.images.reshape((-1, nSteps, nInput))\n",
    "    testLabel = mnist.test.labels\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: testData, y: testLabel}))\n",
    "    print(\"Testing loss:\", sess.run(cost, feed_dict={x: testData, y: testLabel}))\n",
    "    sess.close()\n",
    "\n",
    "# Plot the accuracy and loss under different parameters\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(accuracy_list)), accuracy_list, label='Accuracy')\n",
    "plt.title('Accuracy_RNN_50_neuron')\n",
    "plt.legend()\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1.05)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(loss_list)), loss_list, label='Loss')\n",
    "plt.title('Loss_RNN_50_neuron')\n",
    "plt.legend()\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
